{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchaudio.models import Hypothesis, RNNTBeamSearch\n",
    "\n",
    "\n",
    "from syllabe_vocab import VOCAB_TOKENS, syllbe_vocab_size, BLANK_ID\n",
    "\n",
    "class SampleConfig:\n",
    "    def __init__(self):\n",
    "        self.resamplers = {}\n",
    "        self.mel_spectrograms = {}\n",
    "        self.resample_rate = 16000\n",
    "        self.lowpass_filter_width = 64\n",
    "        self.rolloff = 0.9475937167399596\n",
    "        self.resampling_method = \"kaiser_window\"\n",
    "        self.beta = 14.769656459379492\n",
    "        self.n_fft = 1024\n",
    "        self.n_mels = 80\n",
    "\n",
    "class KinspeakEmformerRNNT(torch.nn.Module):\n",
    "    def __init__(self, target_vocab_size,\n",
    "                 target_blank_id):\n",
    "        super(KinspeakEmformerRNNT, self).__init__()\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.target_blank_id = target_blank_id\n",
    "        self.rnnt = torchaudio.models.emformer_rnnt_base(self.target_vocab_size)\n",
    "        self.loss = torchaudio.transforms.RNNTLoss(reduction=\"sum\", clamp=1.0)\n",
    "\n",
    "    def forward(self, log_mel_spectrograms: torch.Tensor, #log_mel_spectrograms: (N,F,L)\n",
    "                log_mel_spectrogram_lengths: List[int],\n",
    "                target_syllabe_ids:torch.Tensor, target_syllabe_id_lengths:List[int],\n",
    "                target_syllabe_ids_with_eos=True,\n",
    "                target_syllabe_gpt_output = None):\n",
    "        sources = log_mel_spectrograms.transpose(1,2)\n",
    "        source_lengths = torch.tensor(log_mel_spectrogram_lengths).to(sources.device, dtype=torch.int32)\n",
    "        target_syllabe_ids = target_syllabe_ids.to(dtype=torch.int32)\n",
    "        targets = target_syllabe_ids.split(target_syllabe_id_lengths)\n",
    "        targets = pad_sequence(targets, batch_first=True)\n",
    "        target_lengths = torch.tensor(target_syllabe_id_lengths).to(targets.device, dtype=torch.int32)\n",
    "        prepended_targets = targets.new_empty([targets.size(0), targets.size(1) + 1])\n",
    "        prepended_targets[:, 1:] = targets\n",
    "        prepended_targets[:, 0] = self.target_blank_id\n",
    "        prepended_target_lengths = target_lengths + 1\n",
    "        (output, src_lengths, _, __) = self.rnnt(sources, source_lengths, prepended_targets, prepended_target_lengths)\n",
    "        loss = torchaudio.functional.rnnt_loss(output, targets, src_lengths-1, target_lengths, blank=self.target_blank_id, reduction = 'mean', clamp=1.0)\n",
    "        return loss\n",
    "\n",
    "def post_process_hypos(\n",
    "    tokens: List[int], tgt_dict: List[str], lstrip: bool = True,\n",
    ") -> str:\n",
    "    post_process_remove_list = [0,1,2,3,4,6]\n",
    "    filtered_hypo_tokens = [token_index for token_index in tokens[1:] if token_index not in post_process_remove_list]\n",
    "    output_string = \"\".join([tgt_dict[idx] for idx in filtered_hypo_tokens]).replace('|', ' ')\n",
    "    if lstrip:\n",
    "        return output_string.lstrip()\n",
    "    else:\n",
    "        return output_string\n",
    "\n",
    "\n",
    "def _piecewise_linear_log(x):\n",
    "    x[x > math.e] = torch.log(x[x > math.e])\n",
    "    x[x <= math.e] = x[x <= math.e] / math.e\n",
    "    return x\n",
    "\n",
    "\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, tgt_dict: List[str]):\n",
    "        super().__init__()\n",
    "        # self.transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=400, n_mels=80, hop_length=160)\n",
    "        cfg = SampleConfig()\n",
    "        win_length = cfg.resample_rate * 25 // 1000  # 25ms\n",
    "        hop_length = cfg.resample_rate * 10 // 1000  # 10ms\n",
    "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=cfg.resample_rate, n_fft=cfg.n_fft,\n",
    "                                           win_length=win_length,\n",
    "                                           hop_length=hop_length, center=True, pad_mode=\"reflect\", power=2.0,\n",
    "                                           norm=\"slaney\", onesided=True, n_mels=cfg.n_mels,\n",
    "                                           mel_scale=\"htk\", )\n",
    "\n",
    "        rnnt = torchaudio.models.emformer_rnnt_base(syllbe_vocab_size())\n",
    "        model = KinspeakEmformerRNNT(syllbe_vocab_size(), BLANK_ID)\n",
    "\n",
    "        state_dict = torch.load('/home/user/kinspeak_asr_syllabe_emformer_rnnt_base.pt', map_location='cpu')\n",
    "\n",
    "        model.load_state_dict(state_dict['model_state_dict'])\n",
    "        rnnt.load_state_dict(model.rnnt.state_dict())\n",
    "        del state_dict\n",
    "        del model\n",
    "\n",
    "        rnnt.eval()\n",
    "\n",
    "        self.decoder = RNNTBeamSearch(rnnt, BLANK_ID)\n",
    "        \n",
    "        self.decoder.eval()\n",
    "        \n",
    "        self.tgt_dict = tgt_dict\n",
    "    \n",
    "    def feature_extractor(self, input):\n",
    "        log_eps = 1e-36\n",
    "        spectrogram = self.mel_spectrogram(input).transpose(1, 0)\n",
    "        features = torch.log(spectrogram + log_eps)#.unsqueeze(0)[:, :-1]\n",
    "        length = torch.tensor([features.shape[0]])\n",
    "        return features, length\n",
    "\n",
    "    def token_processor(self, hypos, lstrip=False):\n",
    "        transcript = post_process_hypos(hypos, self.tgt_dict, lstrip=lstrip)\n",
    "#         print(f\"\\nTranscript: '{transcript}'\\n\")\n",
    "        return transcript\n",
    "\n",
    "    def infer(self, features, length, state, hypothesis):\n",
    "        # print('\\nfeatures:', features.shape, 'Sum:', features.sum(), 'Min:', features.min(), 'Max:', features.max(), 'Mean:', features.mean(), '\\n')\n",
    "        # print('state:', type(state))\n",
    "        hypos, state = self.decoder.infer(features, length, 8, state=state, hypothesis=hypothesis)\n",
    "        return hypos, state\n",
    "\n",
    "class ContextCacher:\n",
    "    \"\"\"Cache the end of input data and prepend the next input data with it.\n",
    "\n",
    "    Args:\n",
    "        segment_length (int): The size of main segment.\n",
    "            If the incoming segment is shorter, then the segment is padded.\n",
    "        context_length (int): The size of the context, cached and appended.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, segment_length: int, context_length: int):\n",
    "        self.segment_length = segment_length\n",
    "        self.context_length = context_length\n",
    "        self.context = torch.zeros([context_length])\n",
    "\n",
    "    def __call__(self, chunk: torch.Tensor):\n",
    "        if chunk.size(0) < self.segment_length:\n",
    "            chunk = torch.nn.functional.pad(chunk, (0, self.segment_length - chunk.size(0)))\n",
    "        chunk_with_context = torch.cat((self.context, chunk))\n",
    "        self.context = chunk[-self.context_length :]\n",
    "        return chunk_with_context\n",
    "\n",
    "print('Models defined!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae8be0dcf3341d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.io import StreamReader\n",
    "\n",
    "hop_length=160\n",
    "segment_length=16\n",
    "right_context_length=4\n",
    "\n",
    "sample_rate = 16000\n",
    "segment_length = segment_length * hop_length\n",
    "context_length = right_context_length * hop_length\n",
    "\n",
    "print(f\"Sample rate: {sample_rate}\")\n",
    "print(f\"Main segment: {segment_length} frames ({segment_length / sample_rate} seconds)\")\n",
    "print(f\"Right context: {context_length} frames ({context_length / sample_rate} seconds)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0135fbbbd9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# src = \"https://download-a.akamaihd.net/files/media_periodical/be/w_YW_202105_06.mp3\"\n",
    "\n",
    "# Imbogo\n",
    "src = \"https://www.laits.utexas.edu/phonology/sounds/MP3/142b.mp3\"\n",
    "\n",
    "# Abakobwa\n",
    "# src = \"https://www.laits.utexas.edu/phonology/sounds/MP3/007b.mp3\"\n",
    "\n",
    "streamer = StreamReader(src)\n",
    "streamer.add_basic_audio_stream(frames_per_chunk=segment_length, sample_rate=sample_rate)\n",
    "\n",
    "print(streamer.get_src_stream_info(0))\n",
    "print(streamer.get_out_stream_info(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baebe704aa92cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cacher = ContextCacher(segment_length, context_length)\n",
    "\n",
    "wrapper = ModelWrapper(VOCAB_TOKENS)\n",
    "\n",
    "state, hypothesis = None, None\n",
    "\n",
    "print('Wrapper defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c9187ef677f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_iterator = streamer.stream()\n",
    "\n",
    "\n",
    "def _plot(feats, num_iter, unit=25):\n",
    "    unit_dur = segment_length / sample_rate * unit\n",
    "    num_plots = num_iter // unit + (1 if num_iter % unit else 0)\n",
    "    if num_plots == 1:\n",
    "        i = 0\n",
    "        fig, ax = plt.subplots(num_plots, 1)\n",
    "        t0 = 0\n",
    "        feats_ = feats[i * unit : (i + 1) * unit]\n",
    "        t1 = t0 + segment_length / sample_rate * len(feats_)\n",
    "        feats_ = torch.cat([f[2:-2] for f in feats_])  # remove boundary effect and overlap\n",
    "        ax.imshow(feats_.T, extent=[t0, t1, 0, 1], aspect=\"auto\", origin=\"lower\")\n",
    "        ax.tick_params(which=\"both\", left=False, labelleft=False)\n",
    "        ax.set_xlim(t0, t0 + unit_dur)\n",
    "        t0 = t1\n",
    "    else:\n",
    "        fig, axes = plt.subplots(num_plots, 1)\n",
    "        t0 = 0\n",
    "        for i, ax in enumerate(axes):\n",
    "            feats_ = feats[i * unit : (i + 1) * unit]\n",
    "            t1 = t0 + segment_length / sample_rate * len(feats_)\n",
    "            feats_ = torch.cat([f[2:-2] for f in feats_])  # remove boundary effect and overlap\n",
    "            ax.imshow(feats_.T, extent=[t0, t1, 0, 1], aspect=\"auto\", origin=\"lower\")\n",
    "            ax.tick_params(which=\"both\", left=False, labelleft=False)\n",
    "            ax.set_xlim(t0, t0 + unit_dur)\n",
    "            t0 = t1\n",
    "    fig.suptitle(\"MelSpectrogram Feature\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def run_inference(num_iter=100):\n",
    "    global state, hypothesis\n",
    "    chunks = []\n",
    "    feats = []\n",
    "    for i, (chunk,) in enumerate(stream_iterator, start=1):\n",
    "        first = (state is None)\n",
    "        segment = cacher(chunk[:, 0])\n",
    "        if first:\n",
    "            print('segment:', segment.shape, flush=True)\n",
    "            print('Time:', segment.size(0)/sample_rate, 'secs', flush=True)\n",
    "        features, length = wrapper.feature_extractor(segment)\n",
    "\n",
    "        print('features:', features.shape, flush=True)\n",
    "        print('length:', length, flush=True)\n",
    "\n",
    "        hypos, state = wrapper.infer(features, length, state, hypothesis)\n",
    "        if first:\n",
    "            print('hypos:', type(hypos), type(hypos[0]), type(hypos[1]), type(hypos[0][0]))\n",
    "        print('hypos:', len(hypos), len(hypos[0]), len(hypos[1]), len(hypos[0][0]))\n",
    "        hypothesis = hypos\n",
    "        transcript = wrapper.token_processor(hypos[0][0], lstrip=False)\n",
    "        print(f\"\\nTranscript: '{transcript}'\\n\")\n",
    "#         print('chunk:', chunk.shape, flush=True)\n",
    "#         print('segment:', segment.shape, flush=True)\n",
    "#         print('features:', features.shape, flush=True)\n",
    "#         print('features length:', length, flush=True)\n",
    "#         print('Hypos:', hypos[0][0], flush=True)\n",
    "#         print('Hypos:', len(hypos[0][0]), flush=True)\n",
    "#         print('transcript:', transcript, flush=True)\n",
    "#         print(i, transcript[:10], end=\"\\n\", flush=True)\n",
    "        # print(transcript, end=\"\\r\", flush=True)\n",
    "\n",
    "        chunks.append(chunk)\n",
    "        feats.append(features)\n",
    "        if i == num_iter:\n",
    "            break\n",
    "\n",
    "    # Plot the features\n",
    "    _plot(feats, num_iter)\n",
    "    return IPython.display.Audio(torch.cat(chunks).T.numpy(), rate=sample_rate)\n",
    "\n",
    "print('Inference code ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f305edc205389fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_inference(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebaddba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
